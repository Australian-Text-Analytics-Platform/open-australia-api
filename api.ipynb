{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc497bd1",
   "metadata": {},
   "source": [
    "# Using APIs: Open Australia\n",
    "\n",
    "OpenAustralia is a charity run by the OpenAustralia Foundation to help the public keep tabs on their political representatives. Learn more about them by going to: https://www.openaustralia.org.au\n",
    "\n",
    "In this notebook, we will obtain the record of parliamentary debates (Hansard) accessed via OpenAustralia with an API (application programming interface). The mechanisms that allow us to retrieve data in this fashion are the same as those that means a website can be get information to render, which are via HTTP requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c758912",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Skills:</b> \n",
    "    \n",
    "- Retrieving text from a hosted site\n",
    "- APIs, specifically a REST API using GET\n",
    "- Word frequency counts\n",
    "- A simple visual representation of data\n",
    "- Normalisation (lemmatisation)\n",
    "    \n",
    "<b>Skill Level:</b> Beginner/Intermediate\n",
    "</div>\n",
    "\n",
    "<b>NOTE:</b> You may want to learn how to save the downloaded text into the LDaCa repository with this follow-up notebook: https://github.com/Australian-Text-Analytics-Platform/create-rocrate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we begin, let's make sure that we install all the requirements that we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21229a96",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Installing Libraries</b> \n",
    "\n",
    "The requirements file <b>requirements.txt</b> is included with this notebook. Take a look inside to find out what libraries you have just installed with the above command.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5db92e",
   "metadata": {},
   "source": [
    "## Apply for an API Key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541a383",
   "metadata": {},
   "source": [
    "To gain access to the data you will need to apply an API key. To get one, you can follow the instruction on this website: https://www.openaustralia.org.au/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ccb6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b> \n",
    "\n",
    "In this notebook we'll be learning how to use the library 'requests'\n",
    "\n",
    "Learn more about the library here: https://docs.python-requests.org/\n",
    "    \n",
    "It is also assumed that you have some familiarity with dataframes, eg. pandas. If you do not, don't worry, you will still be able to run the notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac2a0a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>You Need an API Key to Run this Notebook</b> \n",
    "\n",
    "In general, you will need to apply for an api key to use an api. The format of the OpenAustralia api key is a string (eg 'RPLDbrHE9cPoEn2MIfQWfRcA') that you will need to paste below as the value for API_KEY.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1936ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants at the very beginning\n",
    "OA_API = 'http://www.openaustralia.org/api/'\n",
    "API_KEY = None  # add your key here as a string, eg 'RPLDbrHE9cPoEn2MIfQWfRcA' (with the quotes)\n",
    "OUTPUT_FORMAT = 'js'\n",
    "\n",
    "if API_KEY is None:\n",
    "    API_KEY = open('oak.txt', 'r').read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb249d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Why Store Constants as Variables?</b> \n",
    "\n",
    "The endpoint we use won't change throughout the notebook once we've defined it, but at some point you may want to reuse this notebook. Having a handle for the api or your api key means that the reuse of this notebook or the code within it won't have to change that much. <b>This is just some good coding hygiene</b>; there is only one place where you need to change a value without any worry.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdae07",
   "metadata": {},
   "source": [
    "### Function: getRepresentatives\n",
    "\n",
    "Each member of parliament has a unique id and we can obtain this unique id by using the function 'getRepresentatives'. We can learn about this function in the website below.\n",
    "\n",
    "    https://www.openaustralia.org.au/api/docs/getRepresentatives\n",
    "    \n",
    "As you can see in the description, all arguments to this function are optional. This function will simply return all members of Parliament and it will be up to us to process the data to find the unique id for the politicians we are interested in.\n",
    "\n",
    "Even though it is stated that all argument are optional, it is implicit that the api_key will is required as a parameter to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of parameters as arguments to the function\n",
    "params = dict()\n",
    "params['key'] = API_KEY\n",
    "params['output'] = OUTPUT_FORMAT\n",
    "\n",
    "# The function we are using is 'getRepresentatives' so we can fetch the politician's member_id\n",
    "my_function = 'getRepresentatives'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee734db",
   "metadata": {},
   "source": [
    "## RESTful API: GET\n",
    "\n",
    "A 'get' request is a method to retrieve data without modifying the originating resource in any way.\n",
    "\n",
    "The library 'requests' in Python has a function 'get', which requires as the first argument the 'endpoint' (or url to retrieve the data) followed by other parameters such as your api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full endpoint is the string: OA_API+my_function. In Python you can concatenate strings with '+' \n",
    "response = requests.get(OA_API+my_function, params=params)\n",
    "result = response.json()\n",
    "\n",
    "print('NUMBER OF PARLIAMENTARIANS:', len(result))\n",
    "\n",
    "# Create variables to store the info from each politician of interest\n",
    "bandt = dict()\n",
    "allen = dict()\n",
    "\n",
    "# Loop through the results to find the relevant information\n",
    "for senator in result:\n",
    "    if senator['last_name'] == 'Bandt':\n",
    "        print('INFO ON BANDT:')\n",
    "        print(senator)\n",
    "        bandt = senator\n",
    "    if senator['last_name'] == 'Allen':\n",
    "        print('INFO ON ALLEN:')\n",
    "        print(senator)\n",
    "        allen = senator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2836acb",
   "metadata": {},
   "source": [
    "### Function: getHansard\n",
    "\n",
    "Now that we have the member_ids for Adam Bandt and Katie Allen, we can retrieve their debates using the getHansard function. \n",
    "\n",
    "Learn more about this function here:\n",
    "\n",
    "    https://www.openaustralia.org.au/api/docs/getHansard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759da93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_function = 'getHansard'\n",
    "my_params = params        # require the old params, such as the api_key and the output type\n",
    "my_params['order'] = 'd'  # d for date; r for relevance; p for person\n",
    "\n",
    "\n",
    "# Allen\n",
    "my_params['person'] = allen['person_id']\n",
    "response = requests.get(OA_API+my_function, params=my_params)\n",
    "hansard_allen = response.json()\n",
    "\n",
    "# Bandt\n",
    "my_params['person'] = bandt['person_id']\n",
    "response = requests.get(OA_API+my_function, params=my_params)\n",
    "hansard_bandt = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830af83c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Inspect Your Output</b> \n",
    "\n",
    "It's always a good idea to inspect your output by printing it to screen, especially if your data is provided by some other third party. This lets you become familiar with the kind of information that's there and it's a good way to see if you've made a mistake somewhere in your code, particularly if you get unexpected results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data on Adam Bandt\n",
    "hansard_bandt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys\n",
    "hansard_allen.keys()\n",
    "hansard_bandt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5021ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the rows\n",
    "hansard_bandt['rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3653c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>List Comprehension</b> \n",
    "\n",
    "List comprehension is a more compact way of iterating over a sequence of items to create a new list. This mechanism is used to create more readable concise code, which can replace lengthly for-loops.\n",
    "\n",
    "Learn more about them here: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the snippets of the debates in appropriately named variables using list comprehension\n",
    "bandt_snippets = [snippet['body'].replace('&#8212', '—') for snippet in hansard_bandt['rows'] if 'body' in snippet]\n",
    "allen_snippets = [snippet['body'].replace('&#8212', '—') for snippet in hansard_allen['rows'] if 'body' in snippet]\n",
    "\n",
    "# Print the first 5 in the list\n",
    "bandt_snippets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129c7ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Try it Yourself:</b> \n",
    "    \n",
    "To see if you understand how the snippets above were saved as lists (bandt_snippets and allen_snippets) using list comprehension, try to recode these lines as a for-loop. Here are some hints:\n",
    "<ul>    \n",
    "    <li> Before starting your \"for-loop\" make sure you iniatialise your variables as empty lists, for example: </li>\n",
    "        <ul>\n",
    "        <li> bandt_snippets = list() </li>\n",
    "        <li> allen_snippets = list() </li>\n",
    "        </ul>\n",
    "    <li> Start your \"for-loop\" with the following:</li>\n",
    "    <ul>\n",
    "        <li> for snippet in \"hansard_bandt['rows']\" </li>\n",
    "    </ul>\n",
    "    <li> When you print \"bandt_snippets[:5]\" above, you should get the same results </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174277ef",
   "metadata": {},
   "source": [
    "## Plotting the Word Count\n",
    "\n",
    "We want to retrieve the debates/speeches from each of the politicians. From having printed the information above, we can see that the data we want to extract is in 'rows', and more specifically under a key called 'body'. This does not have the whole speech, but only the **first 400 characters** or so. The entire speech can be found by following the link in the 'listurl', after prepreding it with the base url. This will however lead us to a website with the speech and therefore the text would have to be scraped. \n",
    "\n",
    "**NOTE:** You can learn how to scrape webpages with this related notebook: https://github.com/Australian-Text-Analytics-Platform/web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aadeba",
   "metadata": {},
   "source": [
    "There are already tools that do words counts based on a list of words of interest, for example the [AntWordProfilier](https://www.laurenceanthony.net/software/antwordprofiler/). However, this tool relies on the user listing all the exact surface word forms. For instance, if you were interested in the word 'look', then this tool would only find the exact match 'look' and not all the morphological forms such as 'looked', 'looking', or 'looks'. We can employ lemmatisers, such as the spaCy lemmatiser, to normalise the word form. You can learn more about lemmatisation and how it's different to stemming in this page: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fb04a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b> \n",
    "\n",
    "- spaCy\n",
    "    - for text cleaning and normalisation\n",
    "- matplotlib\n",
    "    - for rendering a bar chart\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "# Import the library to plot graphs but refer to it throughout the notebook as 'plt' for short\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d8285",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Loading a spaCy Model</b> \n",
    "\n",
    "Allows us to pos tag, parse, and it's all trainable/modifiable, and oh so many languages, blah blah: https://spacy.io/usage/models\n",
    "\n",
    "Helps normalise text (eg lemmatisation, case folding)    \n",
    "    \n",
    "Also introduce stop words here...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bandt = ' '.join(bandt_snippets)\n",
    "all_allen = ' '.join(allen_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_pos = ['PUNCT', 'SYM', 'NUM', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b487c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_bandt = [token.lemma_.lower() for token in nlp(all_bandt) if not token.is_stop and token.pos_ not in omit_pos]\n",
    "tokens_allen = [token.lemma_.lower() for token in nlp(all_allen) if not token.is_stop and token.pos_ not in omit_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_bandt = list(set(tokens_bandt))\n",
    "all_words_allen = list(set(tokens_allen))\n",
    "\n",
    "omit_words = ['australia', 'australian', 'australians', 'minister', 'bill', 'government']\n",
    "\n",
    "count_bandt = [(w, tokens_bandt.count(w)) for w in all_words_bandt if w not in omit_words]\n",
    "count_allen = [(w, tokens_allen.count(w)) for w in all_words_allen if w not in omit_words]\n",
    "\n",
    "count_bandt.sort(key=lambda x: x[1], reverse=True)\n",
    "count_allen.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4832e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bandt = [key[0] for key in count_bandt[:20]]\n",
    "y_bandt = [value[1] for value in count_bandt[:20]]\n",
    "\n",
    "x_allen = [key[0] for key in count_allen[:20]]\n",
    "y_allen = [value[1] for value in count_allen[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeeb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(x_bandt)), y_bandt, align='center')\n",
    "plt.xticks(range(len(x_bandt)), x_bandt)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(x_allen)), y_allen, align='center')\n",
    "plt.xticks(range(len(x_allen)), x_allen)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b154753",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Try it Yourself:</b> \n",
    "    \n",
    "The graphs above show the top 20 words spoken by each politician in the snippets. Try changing this to the top 10 words or the top 30.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4316988",
   "metadata": {},
   "source": [
    "### How Many Documents?\n",
    "\n",
    "A word may appear many times in the document collection because it's mentioned all throughout the collection or perhaps because it's mentioned many times in the the one document.\n",
    "\n",
    "In the following code we uncover the extent of spread of these top words over the document collection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if these words are talked about throughout the collection of snippets retrieved in OpenAustralia\n",
    "\n",
    "# Save the documents (which are actually webpages) in a list\n",
    "# These correspond to bandt_snippets and allen_snippets, \n",
    "# i.e. bandt_docs[n] specifies the website for the speech bandt_snippets[n]\n",
    "bandt_docs = [snippet['gid'] for snippet in hansard_bandt['rows'] if 'body' in snippet]\n",
    "allen_docs = [snippet['gid'] for snippet in hansard_allen['rows'] if 'body' in snippet]\n",
    "\n",
    "# We have an issue: we don't have a set of normalised documents according to their snippets\n",
    "# That is, bandt_snippets and allen_snippets are the raw texts\n",
    "# Let's clean the documents again, but this time we will keep the snippets separate, and we won't\n",
    "# save them as one big collection, like we did for tokens_bandt and tokens_allen. These are a list of \n",
    "# tokens in the whole document collection.\n",
    "\n",
    "# Let's start by defining a function that normalises the text in the snippets \n",
    "def normalise_text(text):\n",
    "    # These are already defined outside of this function and we are using them inside our function\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # omit_pos = ['PUNCT', 'SYM', 'NUM', 'X']\n",
    "    # omit_words = ['australia', 'australian', 'australians', 'minister', 'bill', 'government']\n",
    "    out = list()\n",
    "    for token in nlp(text):\n",
    "        if not token.is_stop and token.pos_ not in omit_pos:\n",
    "            out.append(token.lemma_.lower())\n",
    "    return out\n",
    "\n",
    "# The variables x_bandt and x_allen hold the top 20 words spoken by each individual\n",
    "# Let's save the results in bandt_words_in_docs and allen_words_in_docs\n",
    "bandt_words_in_docs = list()\n",
    "allen_words_in_docs = list()\n",
    "\n",
    "for word in x_bandt:\n",
    "    my_count = 0\n",
    "    for n in range(len(bandt_snippets)):\n",
    "        if word in normalise_text(bandt_snippets[n]):\n",
    "            my_count += 1\n",
    "    bandt_words_in_docs.append((word, my_count))\n",
    "\n",
    "\n",
    "for word in x_allen:\n",
    "    my_count = 0\n",
    "    for n in range(len(allen_snippets)):\n",
    "        if word in normalise_text(allen_snippets[n]):\n",
    "            my_count += 1\n",
    "    allen_words_in_docs.append((word, my_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccf0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "average_bandt = [(count_bandt[c][0],count_bandt[c][1]/bandt_words_in_docs[c][1]) for c in range(20)]    \n",
    "average_allen = [(count_allen[c][0],count_allen[c][1]/allen_words_in_docs[c][1]) for c in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_docs = [value[1] for value in bandt_words_in_docs[:20]]\n",
    "y_avg = [value[1] for value in average_bandt[:20]]\n",
    " \n",
    "plt.bar(x_axis, y_docs, 0.4, label = 'docs')\n",
    "plt.bar(x_axis, y_avg, 0.4, label = 'avg')\n",
    "  \n",
    "plt.xticks(range(len(x_bandt)), x_bandt)\n",
    "plt.xticks(rotation=70)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Word Count\")\n",
    "plt.title(\"Average Word Count per Document\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd415c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_docs = [value[1] for value in allen_words_in_docs[:20]]\n",
    "y_avg = [value[1] for value in average_allen[:20]]\n",
    " \n",
    "plt.bar(x_axis, y_docs, 0.4, label = 'docs')\n",
    "plt.bar(x_axis, y_avg, 0.4, label = 'avg')\n",
    "  \n",
    "plt.xticks(range(len(x_allen)), x_allen)\n",
    "plt.xticks(rotation=70)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Word Count\")\n",
    "plt.title(\"Average Word Count per Document\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
